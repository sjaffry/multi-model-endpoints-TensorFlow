{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d99728",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e92aa3",
   "metadata": {},
   "source": [
    "This is a cut-down version of a notebook created by my colleague Ram Vegiraju (rvegira@amazon.com). To see his full notebook you can checkout his [github page](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/RealTime/Multi-Model-Endpoint/TensorFlow).\n",
    "\n",
    "With Amazon SageMaker [multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "To demonstrate how multi-model endpoints can be created and used, this notebook provides an example using models trained with the [SageMaker TensorFlow framework container](https://github.com/aws/sagemaker-tensorflow-serving-container/tree/dc1ccd1cb19114a0b357862aa2177e9d2a67fdf5). \n",
    "\n",
    "We'll train and deploy two different TensorFlow ANN Models for Boston Housing and [Petrol Consumption](https://www.kaggle.com/harinir/petrol-consumption) datasets. The first portion will cover Boston Housing Steps and then repeat same procedure for the Petrol dataset with its own training script/model.\n",
    "\n",
    "For other MME use cases, you can also refer to:\n",
    "\n",
    "Segmented home value modelling examples with the [PyTorch framework](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_pytorch/pytorch_multi_model_endpoint.ipynb), [Scikit-Learn framework](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_sklearn_home_value), the [XGBoost pre-built algorithm](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_xgboost_home_value), and the [Linear Learner algorithm](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_linear_learner_home_value).\n",
    "An example with [MXNet](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own) and corresponding [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) on how to use MME with your own custom containers.\n",
    "\n",
    "**Kernel**: conda_tensorflow2_p36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee0a3e",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Ensure you take the scripts from the Scripts folder in my repo and move them to a Scripts folder in the base directory of your notebook\n",
    "2. Take the data file petrol_consumption.csv from my repo and add to the base directory of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286c11",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow.serving import TensorFlowModel\n",
    "from sagemaker.multidatamodel import MultiDataModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f67f9",
   "metadata": {},
   "source": [
    "## Role and S3 Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f20e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "TF_FRAMEWORK_VERSION = '2.3.0'\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "PREFIX = 'regression-models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1bfe4",
   "metadata": {},
   "source": [
    "# Boston Housing Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350730f",
   "metadata": {},
   "source": [
    "## Boston Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06241a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets #Boston Housing\n",
    "boston = datasets.load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)\n",
    "y.columns=['TARGET']\n",
    "df = pd.concat([X,y], axis=1)\n",
    "\n",
    "#split into train and test to push to local\n",
    "bostonTrain = df.iloc[:450,:]\n",
    "bostonTest = df.iloc[451:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543281ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './Data/Boston'\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonTrain.to_csv('Data/Boston/train.csv', index=False)\n",
    "bostonTest.to_csv('Data/Boston/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./{DATASET_PATH}/train.csv s3://{BUCKET}/{PREFIX}/BostonHousing/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a38a3",
   "metadata": {},
   "source": [
    "## Create Training Inputs Boston Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(s3_data=f's3://{BUCKET}/{PREFIX}/BostonHousing/train',content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'train': train_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671d957",
   "metadata": {},
   "source": [
    "## Boston Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bostonhousing-ann'\n",
    "hyperparameters = {'epochs': 50}\n",
    "estimator_parameters = {'source_dir':\"Scripts\",\n",
    "                        'entry_point':'boston.py',\n",
    "                        'instance_type': 'ml.m5.2xlarge',\n",
    "                        'instance_count': 1,\n",
    "                        'model_dir': f'/opt/ml/model',\n",
    "                        'role': role,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'output_path': f's3://{BUCKET}/{PREFIX}/BostonHousing/out',\n",
    "                        'base_job_name': f'mme-cv-{model_name}',\n",
    "                        'framework_version': TF_FRAMEWORK_VERSION,\n",
    "                        'py_version': 'py37',\n",
    "                        'script_mode': True}\n",
    "estimator_boston = TensorFlow(**estimator_parameters)\n",
    "estimator_boston.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38d205",
   "metadata": {},
   "source": [
    "## Create Boston Model in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0059c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_model = estimator_boston.create_model(role=role, source_dir=\"Scripts\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffa08a",
   "metadata": {},
   "source": [
    "# Petrol Housing Dataset Training\n",
    "\n",
    "Repeating same process as Boston Housing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65745464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "petrolDF = pd.read_csv(\"petrol_consumption.csv\")\n",
    "petrolTrain = petrolDF.iloc[:35,:]\n",
    "petrolTest = petrolDF.iloc[36:,:]\n",
    "DATASET_PATH = './Data/Petrol'\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "petrolTrain.to_csv('Data/Petrol/train.csv', index=False)\n",
    "petrolTest.to_csv('Data/Petrol/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d81ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./{DATASET_PATH}/train.csv s3://{BUCKET}/{PREFIX}/Petrol/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(s3_data=f's3://{BUCKET}/{PREFIX}/Petrol/train',content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312df426",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'train': train_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ec293",
   "metadata": {},
   "source": [
    "## Petrol Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'petrol-ann'\n",
    "hyperparameters = {'epochs': 50}\n",
    "estimator_parameters = {'source_dir':\"Scripts\",\n",
    "                        'entry_point':'petrol.py',\n",
    "                        'instance_type': 'ml.m5.2xlarge',\n",
    "                        'instance_count': 1,\n",
    "                        'model_dir': f'/opt/ml/model',\n",
    "                        'role': role,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'output_path': f's3://{BUCKET}/{PREFIX}/Petrol/out',\n",
    "                        'base_job_name': f'mme-cv-{model_name}',\n",
    "                        'framework_version': TF_FRAMEWORK_VERSION,\n",
    "                        'py_version': 'py37',\n",
    "                        'script_mode': True}\n",
    "estimator_petrol = TensorFlow(**estimator_parameters)\n",
    "estimator_petrol.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2646a",
   "metadata": {},
   "source": [
    "## Create Petrol Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "petrol_model = estimator_petrol.create_model(role=role, source_dir=\"Scripts\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c55e2",
   "metadata": {},
   "source": [
    "# Multi Model Endpoint Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a074b",
   "metadata": {},
   "source": [
    "### Upload boston model artifact to MME S3 model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "current_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "current_time\n",
    "mme_model_artifacts = f's3://{BUCKET}/{PREFIX}/mme/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_model_artifact = estimator_boston.model_data\n",
    "output_boston = f's3://{BUCKET}/{PREFIX}/mme/boston.tar.gz'\n",
    "!aws s3 cp {boston_model_artifact} {output_boston}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce347f",
   "metadata": {},
   "source": [
    "## Create Multi Data Model\n",
    "\n",
    "Can use boston_model or any model from estimators (in this case only 2) because MME operates in a shared container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff69c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n",
    "                     model_data_prefix=mme_model_artifacts,\n",
    "                     model=boston_model,\n",
    "                     sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96218118",
   "metadata": {},
   "source": [
    "## List which models artifacts are in MME Model Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccc81e",
   "metadata": {},
   "source": [
    "## Deploy MME Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.m5.2xlarge',\n",
    "                       endpoint_name=f'mme-tensorflow-{current_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796b400",
   "metadata": {},
   "source": [
    "## Test MME Boston Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135429fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/Boston/train.csv')\n",
    "test[:1]\n",
    "testX = test.drop(\"TARGET\", axis=1)\n",
    "testX = testX[:2].values.tolist()\n",
    "sampInput = {\"inputs\": testX}\n",
    "sampInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ef1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(data=sampInput, initial_args={'TargetModel': 'boston.tar.gz'})\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb891f",
   "metadata": {},
   "source": [
    "### Upload petrol model artifact to MME S3 model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e639035",
   "metadata": {},
   "outputs": [],
   "source": [
    "petrol_model_artifact = estimator_petrol.model_data\n",
    "output_petrol = f's3://{BUCKET}/{PREFIX}/mme/petrol.tar.gz'\n",
    "!aws s3 cp {petrol_model_artifact} {output_petrol}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1ecec",
   "metadata": {},
   "source": [
    "#### Notice how new models can be dynamically added to the MME endpoint by adding them to the S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67838eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6904b",
   "metadata": {},
   "source": [
    "## Test MME Petrol Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10065f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/Petrol/train.csv')\n",
    "testX = test.drop(\"Petrol_Consumption\", axis=1)\n",
    "testX = testX[:2].values.tolist()\n",
    "sampInput = {\"inputs\": testX}\n",
    "sampInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(data=sampInput, initial_args={'TargetModel': 'petrol.tar.gz'})\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c41d3",
   "metadata": {},
   "source": [
    "## Cleanup - Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
